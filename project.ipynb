{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skimage\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessFromPath(image, listImg, listAns, category):\n",
    "    assert type(image) != None\n",
    "    assert image.shape == (1080, 1920, 3)\n",
    "    image = cv2.resize(image, (28, 28))\n",
    "    assert type(image) == np.ndarray\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = image.astype('float32')\n",
    "    image = image/255.0\n",
    "    listImg.append(image)\n",
    "    listAns.append(category)\n",
    "    return listImg, listAns\n",
    "\n",
    "def captureImgWithPath(path, category):\n",
    "    fileList = os.listdir(path)\n",
    "    imgList = []\n",
    "    ansList = []\n",
    "    for name in fileList:\n",
    "        image_full_path = path + '/' + name\n",
    "        img = cv2.imread(image_full_path)\n",
    "        imgList, ansList = preProcessFromPath(img, imgList, ansList, category)\n",
    "    return imgList, ansList\n",
    "\n",
    "def captureImgWithCam(img):\n",
    "    if img is not None:\n",
    "        imgList, _ = preProcessFromPath(img, [], [], 0)\n",
    "        imgArr = np.array(imgList)\n",
    "        return imgArr\n",
    "    else:\n",
    "        assert(0)\n",
    "    return None\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def computeNextState(listMsd, previousState, threshold):\n",
    "    if len(listMsd) > 4:\n",
    "        listMsd = listMsd[1 : ]\n",
    "    avg = sum(listMsd)/len(listMsd)\n",
    "    if previousState == 'empty' and avg > threshold:\n",
    "        return 'inMotionForward', listMsd\n",
    "    elif previousState == 'inMotionForward' and avg < threshold:\n",
    "        return 'filled', listMsd\n",
    "    elif previousState == 'filled' and avg > threshold:\n",
    "        return 'inMotionBackward', listMsd\n",
    "    elif previousState == 'inMotionBackward' and avg < threshold:\n",
    "        return 'empty', listMsd\n",
    "    else:\n",
    "        return previousState, listMsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1325ec636180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#x, y = unison_shuffled_copies(x, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mansList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mansList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "folders = os.listdir('/Users/23amrutad/Desktop/data')\n",
    "imgList = []\n",
    "ansList = []\n",
    "for folder_name in folders:\n",
    "    path = '/Users/23amrutad/Desktop/data/' + folder_name\n",
    "    folderImgs, folderAns = captureImgWithPath(path, int(folder_name))\n",
    "    imgList = imgList + folderImgs\n",
    "    ansList = ansList + folderAns\n",
    "x = np.array(imgList)\n",
    "y = np.array(ansList)\n",
    "x, y = unison_shuffled_copies(x, y)\n",
    "\n",
    "x = x.reshape(x.shape[0], 28, 28, 1)\n",
    "y = y.reshape(y.shape[0], 28, 28, 1)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/23amrutad/Desktop/data/cup-train-standing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-80c1453d4a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#lists to put the images and corresponding answers into for train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcupImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupAns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptureImgWithPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcup_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkLabelCup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mappleImg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappleAns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaptureImgWithPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapple_train_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkLabelApple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0d7ca2d934a0>\u001b[0m in \u001b[0;36mcaptureImgWithPath\u001b[0;34m(path, category)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcaptureImgWithPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mfileList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mimgList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mansList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/23amrutad/Desktop/data/cup-train-standing'"
     ]
    }
   ],
   "source": [
    "#paths for all of the images\n",
    "\n",
    "cup_train_path = '/Users/23amrutad/Desktop/data/cup-train-standing'\n",
    "cup_test_path = '/Users/23amrutad/Desktop/data/cup-test'\n",
    "apple_train_path = '/Users/23amrutad/Desktop/data/apple-train'\n",
    "apple_test_path = '/Users/23amrutad/Desktop/data/apple-test'\n",
    "\n",
    "#\n",
    "kLabelCup = 0\n",
    "kLabelApple = 1\n",
    "\n",
    "#lists to put the images and corresponding answers into for train and test\n",
    "cupImg, cupAns = captureImgWithPath(cup_train_path, kLabelCup)\n",
    "appleImg, appleAns = captureImgWithPath(apple_train_path, kLabelApple)\n",
    "\n",
    "ImagesTrain = np.vstack((cupImg, appleImg))\n",
    "AnswersTrain = cupAns + appleAns\n",
    "AnswersTrain = np.array(AnswersTrain)\n",
    "print('ImagesTrain shape is ', ImagesTrain.shape)\n",
    "print('AnswersTrain shape is ', AnswersTrain.shape)\n",
    "\n",
    "\n",
    "cupImg, cupAns = captureImgWithPath(cup_test_path, kLabelCup)\n",
    "appleImg, appleAns = captureImgWithPath(apple_test_path, kLabelApple)\n",
    "ImagesTest = np.vstack((cupImg, appleImg))\n",
    "AnswersTest = cupAns + appleAns\n",
    "AnswersTest = np.array(AnswersTest)\n",
    "print('ImageTest shape is ', ImagesTest.shape)\n",
    "print('AnswersTest shape is ', AnswersTest.shape)\n",
    "\n",
    "\n",
    "x_train, y_train = unison_shuffled_copies(ImagesTrain, AnswersTrain)\n",
    "x_test = ImagesTest\n",
    "y_test = AnswersTest\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "424/424 [==============================] - 0s 780us/step - loss: 0.5840 - accuracy: 0.7052\n",
      "Epoch 2/60\n",
      "424/424 [==============================] - 0s 332us/step - loss: 0.3896 - accuracy: 0.8278\n",
      "Epoch 3/60\n",
      "424/424 [==============================] - 0s 328us/step - loss: 0.2959 - accuracy: 0.9033\n",
      "Epoch 4/60\n",
      "424/424 [==============================] - 0s 336us/step - loss: 0.2887 - accuracy: 0.9057\n",
      "Epoch 5/60\n",
      "424/424 [==============================] - 0s 382us/step - loss: 0.2754 - accuracy: 0.9269\n",
      "Epoch 6/60\n",
      "424/424 [==============================] - 0s 369us/step - loss: 0.2285 - accuracy: 0.9245\n",
      "Epoch 7/60\n",
      "424/424 [==============================] - 0s 323us/step - loss: 0.2161 - accuracy: 0.9410\n",
      "Epoch 8/60\n",
      "424/424 [==============================] - 0s 335us/step - loss: 0.1938 - accuracy: 0.9458\n",
      "Epoch 9/60\n",
      "424/424 [==============================] - 0s 325us/step - loss: 0.2206 - accuracy: 0.9599\n",
      "Epoch 10/60\n",
      "424/424 [==============================] - 0s 334us/step - loss: 0.1882 - accuracy: 0.9575\n",
      "Epoch 11/60\n",
      "424/424 [==============================] - 0s 338us/step - loss: 0.1982 - accuracy: 0.9646\n",
      "Epoch 12/60\n",
      "424/424 [==============================] - 0s 333us/step - loss: 0.1798 - accuracy: 0.9623\n",
      "Epoch 13/60\n",
      "424/424 [==============================] - 0s 343us/step - loss: 0.1821 - accuracy: 0.9623\n",
      "Epoch 14/60\n",
      "424/424 [==============================] - 0s 333us/step - loss: 0.1688 - accuracy: 0.9670\n",
      "Epoch 15/60\n",
      "424/424 [==============================] - 0s 326us/step - loss: 0.1803 - accuracy: 0.9599\n",
      "Epoch 16/60\n",
      "424/424 [==============================] - 0s 333us/step - loss: 0.1575 - accuracy: 0.9764\n",
      "Epoch 17/60\n",
      "424/424 [==============================] - 0s 348us/step - loss: 0.1760 - accuracy: 0.9670\n",
      "Epoch 18/60\n",
      "424/424 [==============================] - 0s 538us/step - loss: 0.1920 - accuracy: 0.9575\n",
      "Epoch 19/60\n",
      "424/424 [==============================] - 0s 639us/step - loss: 0.1531 - accuracy: 0.9811\n",
      "Epoch 20/60\n",
      "424/424 [==============================] - 0s 599us/step - loss: 0.1391 - accuracy: 0.9717\n",
      "Epoch 21/60\n",
      "424/424 [==============================] - 0s 556us/step - loss: 0.1632 - accuracy: 0.9858\n",
      "Epoch 22/60\n",
      "424/424 [==============================] - 0s 783us/step - loss: 0.1386 - accuracy: 0.9811\n",
      "Epoch 23/60\n",
      "424/424 [==============================] - 0s 631us/step - loss: 0.1406 - accuracy: 0.9741\n",
      "Epoch 24/60\n",
      "424/424 [==============================] - 0s 595us/step - loss: 0.1590 - accuracy: 0.9599\n",
      "Epoch 25/60\n",
      "424/424 [==============================] - 0s 604us/step - loss: 0.1390 - accuracy: 0.9741\n",
      "Epoch 26/60\n",
      "424/424 [==============================] - 0s 362us/step - loss: 0.1272 - accuracy: 0.9811\n",
      "Epoch 27/60\n",
      "424/424 [==============================] - 0s 332us/step - loss: 0.1527 - accuracy: 0.9646\n",
      "Epoch 28/60\n",
      "424/424 [==============================] - 0s 354us/step - loss: 0.1491 - accuracy: 0.9646\n",
      "Epoch 29/60\n",
      "424/424 [==============================] - 0s 343us/step - loss: 0.1493 - accuracy: 0.9599\n",
      "Epoch 30/60\n",
      "424/424 [==============================] - 0s 333us/step - loss: 0.1327 - accuracy: 0.9741\n",
      "Epoch 31/60\n",
      "424/424 [==============================] - 0s 333us/step - loss: 0.1373 - accuracy: 0.9693\n",
      "Epoch 32/60\n",
      "424/424 [==============================] - 0s 345us/step - loss: 0.1414 - accuracy: 0.9646\n",
      "Epoch 33/60\n",
      "424/424 [==============================] - 0s 341us/step - loss: 0.1269 - accuracy: 0.9811\n",
      "Epoch 34/60\n",
      "424/424 [==============================] - 0s 362us/step - loss: 0.1278 - accuracy: 0.9741\n",
      "Epoch 35/60\n",
      "424/424 [==============================] - 0s 402us/step - loss: 0.1556 - accuracy: 0.9623\n",
      "Epoch 36/60\n",
      "424/424 [==============================] - 0s 399us/step - loss: 0.1357 - accuracy: 0.9741\n",
      "Epoch 37/60\n",
      "424/424 [==============================] - 0s 564us/step - loss: 0.1530 - accuracy: 0.9528\n",
      "Epoch 38/60\n",
      "424/424 [==============================] - 0s 583us/step - loss: 0.1287 - accuracy: 0.9693\n",
      "Epoch 39/60\n",
      "424/424 [==============================] - 0s 604us/step - loss: 0.1314 - accuracy: 0.9717\n",
      "Epoch 40/60\n",
      "424/424 [==============================] - 0s 541us/step - loss: 0.1319 - accuracy: 0.9693\n",
      "Epoch 41/60\n",
      "424/424 [==============================] - 0s 794us/step - loss: 0.1289 - accuracy: 0.9693\n",
      "Epoch 42/60\n",
      "424/424 [==============================] - 0s 660us/step - loss: 0.1352 - accuracy: 0.9646\n",
      "Epoch 43/60\n",
      "424/424 [==============================] - 0s 734us/step - loss: 0.1367 - accuracy: 0.9575\n",
      "Epoch 44/60\n",
      "424/424 [==============================] - 0s 711us/step - loss: 0.1305 - accuracy: 0.9693\n",
      "Epoch 45/60\n",
      "424/424 [==============================] - 0s 679us/step - loss: 0.1305 - accuracy: 0.9599\n",
      "Epoch 46/60\n",
      "424/424 [==============================] - 0s 395us/step - loss: 0.1327 - accuracy: 0.9599\n",
      "Epoch 47/60\n",
      "424/424 [==============================] - 0s 432us/step - loss: 0.1398 - accuracy: 0.9575\n",
      "Epoch 48/60\n",
      "424/424 [==============================] - 0s 443us/step - loss: 0.1277 - accuracy: 0.9623\n",
      "Epoch 49/60\n",
      "424/424 [==============================] - 0s 369us/step - loss: 0.1317 - accuracy: 0.9670\n",
      "Epoch 50/60\n",
      "424/424 [==============================] - 0s 360us/step - loss: 0.1327 - accuracy: 0.9646\n",
      "Epoch 51/60\n",
      "424/424 [==============================] - 0s 346us/step - loss: 0.1056 - accuracy: 0.9764\n",
      "Epoch 52/60\n",
      "424/424 [==============================] - 0s 345us/step - loss: 0.1429 - accuracy: 0.9552\n",
      "Epoch 53/60\n",
      "424/424 [==============================] - 0s 340us/step - loss: 0.1316 - accuracy: 0.9646\n",
      "Epoch 54/60\n",
      "424/424 [==============================] - 0s 359us/step - loss: 0.1293 - accuracy: 0.9670\n",
      "Epoch 55/60\n",
      "424/424 [==============================] - 0s 345us/step - loss: 0.1112 - accuracy: 0.9764\n",
      "Epoch 56/60\n",
      "424/424 [==============================] - 0s 354us/step - loss: 0.1096 - accuracy: 0.9693\n",
      "Epoch 57/60\n",
      "424/424 [==============================] - 0s 351us/step - loss: 0.1130 - accuracy: 0.9717\n",
      "Epoch 58/60\n",
      "424/424 [==============================] - 0s 354us/step - loss: 0.1025 - accuracy: 0.9835\n",
      "Epoch 59/60\n",
      "424/424 [==============================] - 0s 411us/step - loss: 0.1271 - accuracy: 0.9575\n",
      "Epoch 60/60\n",
      "424/424 [==============================] - 0s 388us/step - loss: 0.1126 - accuracy: 0.9788\n",
      "44/44 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0019641184447788296, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    " \n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation=tf.nn.softmax))\n",
    " \n",
    " \n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=60)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/23amrutad/Desktop/newData/image0.jpg\n",
      "it is a cup  [[0.9992721676826477, 0.000727799953892827]]\n",
      "0.9992721676826477\n",
      "/Users/23amrutad/Desktop/newData/image1.jpg\n",
      "it is an apple  [[0.0196347925812006, 0.9803652763366699]]\n",
      "0.0196347925812006\n",
      "/Users/23amrutad/Desktop/newData/image2.jpg\n",
      "it is an apple  [[0.00010817852307809517, 0.9998917579650879]]\n",
      "0.00010817852307809517\n",
      "/Users/23amrutad/Desktop/newData/image3.jpg\n",
      "it is a cup  [[0.995502769947052, 0.004497280344367027]]\n",
      "0.995502769947052\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a0a454946761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrentImg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreviousImg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pathNewData = '/Users/23amrutad/Desktop/newData/image'\n",
    "i = 0\n",
    "\n",
    "\n",
    "previousState = 'empty'\n",
    "previousImg = None\n",
    "currentImg = None\n",
    "cap = cv2.VideoCapture(0)\n",
    "msdList = []\n",
    "\n",
    "while(True):\n",
    "    time.sleep(0.1)\n",
    "    ret, currentImg = cap.read()\n",
    "    if (previousImg is not None):\n",
    "        msd = skimage.measure.compare_mse(currentImg, previousImg)\n",
    "        msdList.append(msd)\n",
    "        state, msdList = computeNextState(msdList, previousState, 500)\n",
    "        #print('the state is ', state)\n",
    "        if previousState != 'filled' and state == 'filled':\n",
    "            image_path = pathNewData + '%s'%i +'.jpg'\n",
    "            print(image_path)\n",
    "            i = i+1\n",
    "            cv2.imwrite(image_path, currentImg)\n",
    "            #print(currentImg.shape, ' is the image shape')\n",
    "            imgList, _ = preProcessFromPath(currentImg, [], [], 0)\n",
    "            imgArr = np.array(imgList)\n",
    "            imgArr = imgArr.reshape(imgArr.shape[0], 28, 28, 1)\n",
    "            prediction = model.predict(imgArr)\n",
    "            predictionList = prediction.tolist()\n",
    "            if predictionList[0][0] > predictionList[0][1]:\n",
    "                print('it is a cup ', predictionList)\n",
    "            elif predictionList[0][1] > predictionList[0][0]:\n",
    "                print('it is an apple ', predictionList)\n",
    "            else:\n",
    "                print('confused ', predictionList)\n",
    "            print(predictionList[0][0])\n",
    "        previousState = state\n",
    "    previousImg = currentImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
